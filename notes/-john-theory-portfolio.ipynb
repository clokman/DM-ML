{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of the following strategies to address missing data:\n",
    "\n",
    "- **Remove any rows** that contain missing data.\n",
    "- Populate the empty fields with a **specified value**.\n",
    "- Populate the empty fields with a **calculated value**.\n",
    "- Use **analysis techniques** that work with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LECTURE NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3\n",
    "2017.04.10\n",
    "- Association rules\n",
    "- Instance-based learning\n",
    "- Cross-validaton\n",
    "\n",
    "### Covering Algorithms (continuation from last week)\n",
    "Different than decision trees\n",
    "- Hard cases vs. Cases\n",
    "\t- Essentially, looking at how many cases are covered by a rule.\n",
    "\t- Good to choose more hard cases (in this particular example), if we want to make a rule for hard cases.\n",
    "\t- This is the explanatory power.\n",
    "\t- After selecting best hard cases, then these are removed and then search is repeated.\n",
    "\n",
    "### Associaltion Rules and Instance-Based Learning\n",
    "- Beer-diaper example\n",
    "- {spagetthi, minced meat}->{basil, cheese} -- not only one thing is used to preduicy, and not only one thing  is prediced!\n",
    "- Rows: Transactions (slide4)\n",
    "- Columns: Products  (slide4)\n",
    "- Amounts may be interesting, but the most interesting informaition is whether it something is bought or not.\n",
    "- Purpose 1: Traditional market basket analysis: \"These items are frequently bought together, so that they should be placed close to each other.\"\n",
    "- Purpose 2: Differential market basket analysis: Per supermarket: \"Why do people buy these two items in supermarket A, but not in supermarket B.\"\n",
    "\n",
    "### Support\n",
    "- Find item sets that happen often\n",
    "- How often do we see a particular combination? (e.g., sushi, wasabi, x, y)\n",
    "- e.g., In windy/humididy/play example (slide 9):\n",
    "\t- Rule: If humidity=normal & play=yes, then windy = false\n",
    "\t- Confidence 4/14 (4 over 14)â€”that is, there are 4 cases (rows) that supports all element this rule\n",
    "\n",
    "### Confidence\n",
    "- Find rules that are accurate \n",
    "- Relates to how accurate a rule is.\n",
    "- E.g., 85 percent of the time, wasabi and sushi is baught together\n",
    "- e.g., In windy/humididy/play example (slide 9):\n",
    "\t- Rule: If humidity=normal & play=yes, then windy = false\n",
    "\t- Confidence: 4 out of 6 (???) [see slide 9 to understand]\n",
    "\n",
    "#### Formally (slide 7, slide 8)\n",
    "- items = {ii,.....in}\n",
    "- itemset = {i4, i7}\n",
    "- k itemset = 2 (same with len())\n",
    "- t: transaction\n",
    "- We have n of these items\n",
    "- question: how often {i4, i7} occur in our dataset?\n",
    "\n",
    "### Issues with Association Rules\n",
    "- Datasets can be very big\n",
    "- Only rules with high support and confidence matter\n",
    "- We don't care about things that happen once in a million times\n",
    "- Strategy: first, find all frequent items, and then find among these those sets  with high confidence.\n",
    "\n",
    "\n",
    "### Very similar items\n",
    "- Heineken or beer when buying pizza or Hawaii pizza?\n",
    "\n",
    "### Instance-Based Learning\n",
    "Other names: \n",
    "- Case-based\n",
    "- Memory-based \n",
    "- Non-parametric: no need to approximate parameters\n",
    "- Instead of building a model...\n",
    "\n",
    "### Nearest Neighbour Classifier\n",
    "- Predicts instances based on Euclidean distance\n",
    "- k-nearest neighbour video by Antal...\n",
    "- How do you categorize categorical things?\n",
    "\t- Euclidian distance could be used to assign weigts when classifying\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "#### Crossvalidation\n",
    "- 2/3 as training set, the rest is used as test set.\n",
    "- If data is large enough -> 3 sets:  training, validation, test\n",
    "- If data is not small -> 2 sets: training, test (not preferrable, but OK)\n",
    "\n",
    "#### K-fold cross validation\n",
    "- Divide the data set to 5 parts.\n",
    "- Make different configurations between these 5 parts.\n",
    "- Use 4, test on 1. \n",
    "- Do this 5 times for each different configuration.\n",
    "- Which fold to use for the model?: (If you used a decision tree, averaging may not be possible).\n",
    "\t- Use the best model\n",
    "\t- Afterwards, use the whole data set to test the model.\n",
    "\n",
    "#### Leave-one-out Cross-validation (slide 29)\n",
    "- Costly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
